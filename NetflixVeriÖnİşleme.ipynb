{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NetflixVeriÖnİşlemeFinal.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWDV7hppIflu"
      },
      "source": [
        "import pandas as pd #pandas kütüphanemizi ,verilerimizi okumak için projemize dahil ediyoruz.\n",
        "import numpy as np #Veriler üzerinde işlem yapmamızı sağlayan Numpy kütüphanemizi projemize dahil ediyoruz.\n",
        "\n",
        "dataset=pd.read_csv('/content/netflix_titles.csv') #pandas ile data setimizi içeri aktarıyoruz.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EamhTScjQfWh"
      },
      "source": [
        "df=df=pd.DataFrame(dataset) #Verimizi daha kolay işleybilmek için pandas fonksiyonu olan dataframe yapısını kullanıyoruz.\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qq1u345Xc-F"
      },
      "source": [
        "df.info()  #Pandas'ın info komutu ile datasetimizde bulunan verilerin tiplerini öğreniyoruz.\n",
        "#Burada görüdüğümüz object değişkeni pandasta \"string\" tipini temsil ediyor.\n",
        "#Ayrıca satırda 1 adet bile int olmayan veri oladuğunda satır tipini object olarak görürüz ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjS8t-8DXdDA"
      },
      "source": [
        "df.isnull() #NaN verileri görmek için pandas kütüpanesinin isna() ya da isnull() fonksiyonunu kullanıyoruz.Burda gördüğümüz true değerler NaN değerlerimizi temsil ediyor.\n",
        "df.isnull().sum() #isnull komutumuza bir kere 'sum()' eklediğimizde kolonlardaki \n",
        "df.isnull().sum().sum() # iki defa 'sum()' eklediğimizde ise bize tüm datasetimizde ki toplam NuN sayısını gösteriyor.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEKTk4j_Qfji"
      },
      "source": [
        "df.isnull().sum() #Data frame komutlarından isna komutumuzu kullanıp,bir kere 'sum()' eklediğimizde kolonlardaki NuN sayısını gösterir."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFUJwSvZQf8g"
      },
      "source": [
        "df.nunique() #satırlarımızda uniq verileri görmek için pandasın nunique() komutunu kullanıyoruz."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GWjiU1aQf26"
      },
      "source": [
        "df_2= df.copy() #burada datasetimi işlemlerden önce kopyalıyoruz ki yapıcağımız değişikliklerden yada hatalardan geri dönüşümüz daha rahat olsun.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNMRnKNBQfx4"
      },
      "source": [
        "from sklearn.impute import SimpleImputer #Burada ise makine öğrenmesinde çokça kullandığımız kütüphanelerden olan scikit learn kütüphanesinin SimpleImputer modulu ile eksik verilerimizi dönüştürücez."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlvikCmEQfu8"
      },
      "source": [
        "imp_freq=SimpleImputer(missing_values=np.nan, strategy='most_frequent') #burada ise Numpy'ın eksik veriler için olan Imputer modülündenü kullanarak imp_freq adında bir fonksiyon oluşturuyoruz.\n",
        "#burada gödüğümüz fonksiyonun aldığı değişkenlerden missing values ile hangi tipteki verileri hedef aldığımızı belirliyoruz.Biz burda NaN veri tipleri üzerinde işlem yapıcaz.\n",
        "#burda gördüğümüz strategy ise biz most_frequent stratejisini kullanıyoruz yani satır boyunca hangi veri en çok tekrar ediyorsa NaN verilerimizi ona göre güncelliyoruz."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "driUTXGlQfsX"
      },
      "source": [
        "df_2[\"date_added\"]=imp_freq.fit_transform(df_2[['date_added']]) #Burada data_added kolonuma imp_freq fonksiyonu uyguluyorum(fit ediyorum).\n",
        "df_2[\"rating\"]=imp_freq.fit_transform(df_2[['rating']]) #Burada ise rating kolonuma imp_freq fonksiyonu uyguluyorum(fit ediyorum).\n",
        "#bu iki kolonu secmemin sebebi ise rating ve data_edded tarilerinin benim proem için asında çok önemli olmamasıdır."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWMOVc52Qfpi"
      },
      "source": [
        "df_2.isnull().sum() #NuN değerlerimde iyleştirme yaptıktan sonra Datamın son NuN değerlerine bakıyorum ve data_added ve rating değerlerimin düzeldiğini görüyorum."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GMUIyBLh6OT"
      },
      "source": [
        "df_2[\"country\"] = df['country'].replace(np.nan,\"USA\") #Burada ise country sutunuma, replace fonksiyonunu kullanarak direk müdahale ederek NaN verilerimi 'USA' olarak güncelliyorum\n",
        "#Burda sehirin sayısını kayıp emektense degişken adını değiştirmek ileride yaptığım projede görsel sonuçlarda çok daha iyi sonuç vericektir.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toKI45wHqPLW"
      },
      "source": [
        "df_2[\"director\"] = df['director'].replace(np.nan,\"diger\") #Yine verileri tamamen kaybetmektense bilgi değiştirmek çok daha verimli bir yöntem olucaktır bizim için"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NiIKJntseQm"
      },
      "source": [
        "df_2[\"cast\"] = df['cast'].replace(np.nan,\"bilinmiyor\")#Yine verileri tamamen kaybetmektense bilgi değiştirmek çok daha verimli bir yöntem olucaktır bizim için"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEmJcwMth6R2"
      },
      "source": [
        "df_2.isnull().sum() #Son NuN kontrolumu yaptığımdaysa verimde NuN veri kalmadığını görüyorum."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM3VRJjWyPOE"
      },
      "source": [
        "df_2.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvUopOD4tq1Q"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder # Burada ise scikit learn kütüphanesinden label encoding'i import ediyoruz.label ncodin'i kategorik  değerlerin sayıyla ifade edilmesi için kullandığımız bir kütüphane\n",
        "le = LabelEncoder() #label encoder'a kısaca le olarak çağırıcaz\n",
        "from sklearn.preprocessing import OneHotEncoder #one hot encoder ise aslında yine verimizi nümerik olarak çevirmemize yardımcı olucak.\n",
        "from sklearn.model_selection import train_test_split #Burda ise verimizi train ve test olarak ölmek için ilgili kütüphanemizi çağırıyoruz."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE4pifQYtqq6"
      },
      "source": [
        "le.fit(df_2[\"type\"]) #burada iste type sutunumuza lebelEncoderımızı fit ediceğimi belitiyoruz.\n",
        "list(le.classes_) #burada kaç etiketin nümeriğe çeviriliceğini ve onları listeliyoruz. \n",
        "df_2[\"type\"]=le.transform(df_2[\"type\"]) #burda ise nümeriğe çevirdiğimiz verimizi ilgili sutuna gönderiyoruz."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o23_3OcvTOBa"
      },
      "source": [
        "X=df_2.drop(['show_id'],axis=1) #eğitim verilerimi X olarak verdim\n",
        "y=df_2[\"show_id\"] #Etiket verilerimi y olarak verdim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Nmj3Dp5hRJ"
      },
      "source": [
        "X_encoded = pd.get_dummies(X, prefix_sep=\"_\") # burada pandasın get_dummies fonksiyonu ile nümerik sayılara dönüşüm yaptım\n",
        "# X_endcoded adında bir değişke bu dönüşümü atadım.burda kullandığım  prefix_sep değişkeni ise kolonlar bölündüğünde ayırt etmede bize yardımcı olucak.\n",
        "y_encoded= LabelEncoder().fit_transform(y) #burada yine labelEncoder'ımı kullanarak etiketlerimi nümerik yaptım.Burda kullandığımız fit_transform veriyi ilk olarak fit edip sonra dönüşüm yapıyor."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud35j-P4Uecg"
      },
      "source": [
        "X_encoded #ekrana yazdırma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcPr6cX6Ueaa"
      },
      "source": [
        "y_encoded #ekrana yazdırma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32ncZgcEUeX7"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler #label encoding -> tekil olan kategorik  değerlerin sayıyla ifade edilmesi için kullandığımız bir kütüphane.\n",
        "X_scaled =StandardScaler().fit_transform(X_encoded) #modelin performanısını arttırıp,boyut büyümesini engellemek için standardScaler ile ölçeklendirme yapıyoruz."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abwXfC83UeVA"
      },
      "source": [
        "X_scaled #ekrana yazdırma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Svj6n_UeSu"
      },
      "source": [
        "x_train, x_test,y_train,y_test = train_test_split(X_scaled,y_encoded, test_size=0.3 ,random_state=101) \n",
        "#Burda ise eğitim ve test verilerimiz için verisetimizi bölüyoruz.test_size değişkeni ile data setimizin yüzde 30'nu test  70'ini eğitim verisi olarak ayarlıyoruz.\n",
        "#burda verdiğimiz random_state ise verimizden alınan test ve eğitim datalarının random olarak alınmasını sağlıyor."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm0F5KlzWOvO"
      },
      "source": [
        "print(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfvo8dZVWO2h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hkVoVtcWO_B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61LnTYMbWO8Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}